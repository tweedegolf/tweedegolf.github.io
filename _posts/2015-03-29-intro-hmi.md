###Interacting with a computer

The interaction between a human and a computer, also called human-machine interaction (HMI) or human-computer interaction (HCI) has changed quite a lot in the past decades. Virtual reality (VR) and augmented reality (AR) have received a revived interest because of the development of devices like the Oculus Rift and Microsoft's Hololens. This considering, HCI will probably change even more radically in the coming years.

<br>
**Short history**

HCI has been a topic of active research for decades; researchers and artists have invented the most exotic technologies, for instance Char Davies' art project [Osmose](http://www.immersence.com/osmose/) whereby the user can navigate by breathing and moving her body.

Obviously, not every technology made it to the consumer market, but most technologies that we use today have been invented long before they became mainstream. There are for instance striking similarities between Google Glass and [EyeTap](http://en.wikipedia.org/wiki/EyeTap) developed by Steve Mann in the 1980's and 1990's. Wearable computing dates even further back, to the 17th century when the Chinese invented the [Abacus ring](http://gizmodo.com/this-wearable-abacus-is-basically-the-worlds-oldest-sm-1545627562).

We have come a long way since the interaction with punched cards in the early days. In the 1960's the user interaction happened mostly via the command-line interface (CLI) and although the mouse was already invented in [1964](http://gajitz.com/on-the-origin-of-mouse-first-mouse-nearly-lost-to-history/), it became only mainstream with the advent of the graphical user interface (GUI) in the early 1980's. GUI's also made it more apparent that HCI is actually a two-way communication; the computer receives its input via the GUI and gives back the output or the feedback via the GUI as well.


<br>
**NUI and gestures**

Speech control became consumer-ready the 1990's (though very expensive back then). Interesting about speech control is that it is the first appearance of Natural User Interaction (NUI). NUI roughly means that the interface is so natural that the user hardly notices it. Another example of NUI is touchscreen interaction, though we have to distinguish between using touch events as replacement for mouse clicks, such as tapping on button element in the GUI, and gestures, for instance a pinch gesture to scale a picture. The latter is NUI, the former is a touch-controlled GUI.

Instead of making gestures on a touch screen, you can also perform them in the air in front of a camera or a controller such as the [Leap Motion](https://www.leapmotion.com/). Gestures can also be made while wearing a [data glove](https://www.vrealities.com/products/data-gloves)

<br>
**AR and VR**

With AR a digital overlay is superimposed on the real world whereas with VR the real world is completely replaced by a virtual (3D) world. Google Glass and Hololens are examples of AR devices. The Oculus Rift and Google Cardboard are examples of VR devices.

Google Glass renders a small display in front of your right eye and the position of this display in relation to your eye doesn't change if you move your head. Hololens on the other hand actually 'reads' the objects in the real world and is able to render digital layers on top of these objects. If you move your head, you'll see both the real world object and the rendered layer from a different angle.

AR is very suitable for creating a reality user interface (RUI), also called a reality based interface (RBI). In a RBI real world objects become actuators; for instance a light switch becomes a button that can be 'clicked' with a certain gesture. An older and more familiar example of RBI is when a 3D scene is rendered on top of a marker; when you rotate the marker in the real world, the 3D scene will rotate accordingly. Instead of a marker you can also use other real world entities, for instance Layar makes use of the GPS data of a mobile device.

In 2013 we have thoroughly researched and subsequently implemented touchscreen based gestures in our 3D framework. This year we take our research a step further by looking into 2 VR devices: the Oculus Rift and Google Cardboard. In the coming blog posts we will share with you the results of this research.


<br>
**Links**

- [NUI](http://en.wikipedia.org/wiki/Natural_user_interface)
- [Wearables](http://en.wikipedia.org/wiki/Wearable_computer)
- [Osmose](http://www.immersence.com/osmose/)
- [Multitouch](http://www.ted.com/talks/jeff_han_demos_his_breakthrough_touchscreen?language=en#t-10169) The video is made in 2006: note how enthusiastic the audience is about multi touch control, nowadays multi touch control is part of our daily life.



<!--
A mouse click on a button in a GUI can be seen as a shortcut for a command.

covers the two-way communication between a human and a machine. Usually the human gives the machine an instruction and the machine provides the human with feedback as soon as the instruction has been processed.
-->
